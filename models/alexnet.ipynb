{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CNN AlexNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary_categorical(array):\n",
    "    new_array = []\n",
    "\n",
    "    for i in range(len(array)):\n",
    "        content = [0] * 25\n",
    "        index = np.argmax(array[i])\n",
    "        content[index] = 1\n",
    "        new_array.append(content)\n",
    "\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.genfromtxt('../dataset/sign_mnist_train.csv', delimiter=',')\n",
    "test_data = np.genfromtxt('../dataset/sign_mnist_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 784) (27455,)\n",
      "(7172, 784) (7172,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data[1:, 1:]\n",
    "y_train = train_data[1:, 0]\n",
    "\n",
    "X_test = test_data[1:, 1:]\n",
    "y_test = test_data[1:, 0]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 12:36:06.423434: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 224, 224, 1)\n",
      "(7172, 224, 224, 1)\n"
     ]
    }
   ],
   "source": [
    "# normalise pixel values from 0-255 to 0-1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# reshape 1d array of 784 to 2d array of size 28x28\n",
    "X_train = X_train.reshape(len(X_train), 28, 28)\n",
    "X_test = X_test.reshape((len(X_test)), 28, 28)\n",
    "\n",
    "# expand 28x28 to 224x224\n",
    "X_train = tf.constant(X_train)\n",
    "X_train = X_train[..., tf.newaxis]\n",
    "\n",
    "X_test = tf.constant(X_test)\n",
    "X_test = X_test[..., tf.newaxis]\n",
    "\n",
    "X_train = tf.image.resize(X_train, [224, 224]).numpy()\n",
    "X_test = tf.image.resize(X_test, [224, 224]).numpy()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(input_shape=(224, 224, 1), kernel_size=(11, 11), filters=96, strides=(4, 4), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(11, 11), strides=(1, 1), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=4096, input_shape=(224 * 224 * 3, ), activation='relu'))\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(units=4096, activation='relu'))\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(units=25, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_crossentropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation using k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687/687 [==============================] - 820s 1s/step - loss: 1.2156 - categorical_crossentropy: 1.2156\n",
      "687/687 [==============================] - 2798s 4s/step - loss: 0.2349 - categorical_crossentropy: 0.2349\n",
      "687/687 [==============================] - 2147s 3s/step - loss: 0.1515 - categorical_crossentropy: 0.1515\n",
      "687/687 [==============================] - 2104s 3s/step - loss: 0.1318 - categorical_crossentropy: 0.1318\n",
      "687/687 [==============================] - 1083s 2s/step - loss: 0.1224 - categorical_crossentropy: 0.1224\n",
      "[0.6800218539428156, 0.9519213258058642, 0.9748679657621563, 0.8672372973957385, 0.9969040247678018]\n",
      "Average F1 Score: 0.8941904935348752\n"
     ]
    }
   ],
   "source": [
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_train, y_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    # one-hot\n",
    "    y_train_fold = to_categorical(y_train_fold)\n",
    "    y_test_fold = to_categorical(y_test_fold)\n",
    "\n",
    "    # fit and predict\n",
    "    model.fit(x=X_train_fold, y=y_train_fold)\n",
    "    y_predicted = model.predict(x=X_test_fold)\n",
    "\n",
    "    # convert predicted float values to binary values\n",
    "    y_predicted = to_binary_categorical(y_predicted)\n",
    "    \n",
    "    # calculate score\n",
    "    score = f1_score(y_test_fold, y_predicted, average='micro')\n",
    "    f1_scores.append(score)\n",
    "\n",
    "print(f1_scores)\n",
    "print('Average F1 Score:', mean(f1_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9191299498047965\n"
     ]
    }
   ],
   "source": [
    "y_test = to_categorical(y_test)\n",
    "\n",
    "y_predicted = model.predict(x=X_test)\n",
    "y_predicted = to_binary_categorical(y_predicted)\n",
    "\n",
    "score = f1_score(y_test, y_predicted, average='micro')\n",
    "print('F1 Score:', score)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
